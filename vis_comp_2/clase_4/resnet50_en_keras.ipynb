{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNets\n",
    "\n",
    "* Vimos que entrenar redes muy profundas presenta dificultades:\n",
    " * Una son los gradientes que tienden a cero o explotan (vanishing y exploding gradients).\n",
    " * La otra es la degradación de la performance en el training set.\n",
    "* Una de las técnicas para resolverlo son las redes residuales.\n",
    "* Vamos a construir los bloques para armar una ResNet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En las ResNets las 'skip connections' o 'shortcuts' o 'residual connections' permiten 'saltar capas'\n",
    "a \"shortcut\" or a \"skip connection\" allows the model to skip layers:  \n",
    "\n",
    "<img src=\"skip_connection_kiank.png\" style=\"width:650px;height:200px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : Un bloque ResNet donde se exhibe una **skip-connection** <br> </center></caption>\n",
    "\n",
    "La imagen a la izquierda muestra el camino principal de la red. La imagen de la derecha agrega un 'shortcut' al camino principal. Apilando estos bloques podemos armar redes con arquitecturas muy profundas.\n",
    "\n",
    "Agregar un bloque adicional con una conexión residual hace muy fácil imitar la función identidad, por lo tanto agregar bloques residuales agrega poco riesgo de degradar la performance en el set de entrenamiento.\n",
    "    \n",
    "Hay dos bloques principales, el que se usa depende de si la dimensión de salida del bloque es la misma que la de entrada o no:\n",
    "    * El bloque con tamaño de entrada = tamaño de salida se llama bloque identidad (identity block).\n",
    "    * El bloque con entrada != tamaño se llama bloque convolucional (convolutional block).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloque identidad\n",
    "\n",
    "El bloque identidad es un bloque estándar utilizado en ResNets, y corresponde al caso en que la activación de entrada (por ej. $a^{[l]}$)  tiene la misma dimensión a la activación de salida ($a^{[l+2]}$). Para esquematizar los diferentes pasos de lo que ocurre en un bloque de identidad de una ResNet, el siguiente diagrama muestra los pasos indivuales:\n",
    "\n",
    "<img src=\"idblock2_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 1** </u><font color='purple'>  : **Bloque Identidad.** Skip connection \"salta\" 2 capas. </center></caption>\n",
    "\n",
    "El camino superior es el 'atajo' o 'shortcut'. El camino superior es el 'camino principal'. En este diagrama se han hecho explícitos el paso convolucional y la no-linearidad ReLU. Para acelerar el entrenamiento también agregamos un paso de Batch Normalization. \n",
    "    \n",
    "En el ejercicio de abajo hay que implementar una versión un poco más poderosa del bloque identidad, en el cual 'saltamos' conexiones sobre 3 capas ocultas en vez de 2. Se ve así:\n",
    "\n",
    "<img src=\"idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2** </u><font color='purple'>  : **Bloque Identidad.** Skip connection \"salta\" 3 capas.</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import backend\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"\n",
    "    El bloque identidad tiene el tamaño del input igual al tamaño del output\n",
    "    \n",
    "    # Argumentos\n",
    "        input_tensor: tensor de entrada\n",
    "        kernel_size: el default es 3, tamaño del kernel de la capa media en el camino principal\n",
    "        filters: lista de enteros, cantidad de filtros de las 3 capas CONV en el camino principal\n",
    "        stage: entrada, rótulo de la etapa actual, usado para generar los nombres de las capas\n",
    "        block: 'a', 'b'..., rótulo del bloque actual, usado para generar nombres de capas\n",
    "        \n",
    "    # Retorna\n",
    "        Tensor de salida del bloque.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch' # nombre base para las capas convolucionales\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch' # nombre base para las capas residuales\n",
    "\n",
    "    # sub-bloque a\n",
    "    # para los nombres utilizar conv_name_base + '2a' para capas conv y bn_name_base + '2a' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> (1,1), # de filtros filters1\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    # esta parte YA fue implementada\n",
    "    \n",
    "    x = layers.Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # sub-bloque b\n",
    "    # para los nombres utilizar conv_name_base + '2b' para capas conv y bn_name_base + '2b' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> el parametro kernel_size\n",
    "    # # de filtros filters2, padding=\"same\"\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # sub-bloque c\n",
    "    # para los nombres utilizar conv_name_base + '2c' para capas conv y bn_name_base + '2c' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> (1,1), # de filtros filters3\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # sumar el tensor de entrada al resultado de las anteriores operaciones (usar la función layers.add(...))\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # retornar el tensor resultante\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"identity_block\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           [(None, 4, 4, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a (Conv2D)         (None, 4, 4, 2)      14          input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2a (BatchNormalizati (None, 4, 4, 2)      8           res1a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 4, 4, 2)      0           bn1a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2b (Conv2D)         (None, 4, 4, 4)      36          activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2b (BatchNormalizati (None, 4, 4, 4)      16          res1a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 4, 4, 4)      0           bn1a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2c (Conv2D)         (None, 4, 4, 6)      30          activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2c (BatchNormalizati (None, 4, 4, 6)      24          res1a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 4, 4, 6)      0           bn1a_branch2c[0][0]              \n",
      "                                                                 input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 4, 4, 6)      0           add_21[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 128\n",
      "Trainable params: 104\n",
      "Non-trainable params: 24\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "A_prev = layers.Input([4, 4, 6])\n",
    "X = np.random.randn(3, 4, 4, 6)\n",
    "A = identity_block(A_prev, 2, [2, 4, 6],1 ,'a')\n",
    "model = models.Model(A_prev, A, name='identity_block')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Salida esperada:\n",
    "\n",
    "Model: \"identity_block\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_27 (InputLayer)           [(None, 4, 4, 6)]    0                                            \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2a (Conv2D)         (None, 4, 4, 2)      14          input_27[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2a (BatchNormalizati (None, 4, 4, 2)      8           res1a_branch2a[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "activation_64 (Activation)      (None, 4, 4, 2)      0           bn1a_branch2a[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2b (Conv2D)         (None, 4, 4, 4)      36          activation_64[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2b (BatchNormalizati (None, 4, 4, 4)      16          res1a_branch2b[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "activation_65 (Activation)      (None, 4, 4, 4)      0           bn1a_branch2b[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2c (Conv2D)         (None, 4, 4, 6)      30          activation_65[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2c (BatchNormalizati (None, 4, 4, 6)      24          res1a_branch2c[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "add_21 (Add)                    (None, 4, 4, 6)      0           bn1a_branch2c[0][0]              \n",
    "                                                                 input_27[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "activation_66 (Activation)      (None, 4, 4, 6)      0           add_21[0][0]                     \n",
    "==================================================================================================\n",
    "Total params: 128\n",
    "Trainable params: 104\n",
    "Non-trainable params: 24\n",
    "__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloque convolucional\n",
    "\n",
    "El bloque convolucional de la ResNet es el segundo tipo de bloque. Se puede usar este tipo de bloque cuando las dimensiones de la entrada y de la salida no coinciden.\n",
    "\n",
    "\n",
    "<img src=\"convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 3** </u><font color='purple'>  : **Bloque convolucional** </center></caption>\n",
    "\n",
    "* La capa CONV2D en el 'shortcut' es usada para redimensionar la entrada $x$ a una dimensión diferente, de tal manera que las dimensiones coinciden en la suma final necesaria para sumar el la skip connection al 'flujo' principal.\n",
    "* Por ejemplo, para reducir la altura y ancho de las activaciones por un factor de 2, podemos usar una convolución 1x1 con un stride de 2.\n",
    "* La capa CONV2D en el la skip connection no usa ninguna función no-lineal. Su rol principal es simplemente aplicar una función lineal que reduce la dimensión de la entrada para que coincidan las dimensiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"Un bloque que tiene una capa convolucional como shortcut.\n",
    "\n",
    "    # Argumentos\n",
    "        input_tensor: tensor de entrada\n",
    "        kernel_size: el default es 3, tamaño del kernel de la capa media en el camino principal\n",
    "        filters: lista de enteros, cantidad de filtros de las 3 capas CONV en el camino principal\n",
    "        stage: entrada, rótulo de la etapa actual, usado para generar los nombres de las capas\n",
    "        block: 'a', 'b'..., rótulo del bloque actual, usado para generar nombres de capas\n",
    "        strides: Strides para la primera capa convolucional del bloque.\n",
    "\n",
    "    # Retorna\n",
    "        Tensor de salida del bloque.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # sub-bloque a\n",
    "    # para los nombres utilizar conv_name_base + '2a' para capas conv y bn_name_base + '2a' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> (1,1)\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # sub-bloque b\n",
    "    # para los nombres utilizar conv_name_base + '2b' para capas conv y bn_name_base + '2b' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> el parametro kernel_size\n",
    "    # # de filtros filters2, padding=\"same\"\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # sub-bloque c\n",
    "    # para los nombres utilizar conv_name_base + '2c' para capas conv y bn_name_base + '2c' \n",
    "    # agregar una capa Conv2D, usar he_normal como kernel_initializer, kernel_size -> (1,1), # de filtros filters3\n",
    "    # agregar Batch Normalization\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # conexión shortcut:\n",
    "    # agregar una capa convolucional que actúe sobre el tensor de entrada\n",
    "    # agregar Batch Normalization\n",
    "    \n",
    "    # aplicar la conexión shortcut al tensor de entrada \n",
    "    # y sumarle a esto el resultado de aplicar las anteriores operaciones (el camino principal) \n",
    "    # a la entrada\n",
    "    # agregar activación ReLU\n",
    "    \n",
    "    # retornar el tensor resultante\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_block\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           [(None, 4, 4, 6)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2a (Conv2D)         (None, 2, 2, 2)      14          input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2a (BatchNormalizati (None, 2, 2, 2)      8           res1a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 2, 2, 2)      0           bn1a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2b (Conv2D)         (None, 2, 2, 4)      36          activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2b (BatchNormalizati (None, 2, 2, 4)      16          res1a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 2, 2, 4)      0           bn1a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch2c (Conv2D)         (None, 2, 2, 6)      30          activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res1a_branch1 (Conv2D)          (None, 2, 2, 6)      42          input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch2c (BatchNormalizati (None, 2, 2, 6)      24          res1a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn1a_branch1 (BatchNormalizatio (None, 2, 2, 6)      24          res1a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 2, 2, 6)      0           bn1a_branch2c[0][0]              \n",
      "                                                                 bn1a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 2, 2, 6)      0           add_22[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 194\n",
      "Trainable params: 158\n",
      "Non-trainable params: 36\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "A_prev = layers.Input([4, 4, 6])\n",
    "X = np.random.randn(3, 4, 4, 6)\n",
    "A = conv_block(A_prev, 2, [2, 4, 6],1 ,'a')\n",
    "model = models.Model(A_prev, A, name='conv_block')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Salida esperada:\n",
    "\n",
    "Model: \"conv_block\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "input_28 (InputLayer)           [(None, 4, 4, 6)]    0                                            \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2a (Conv2D)         (None, 2, 2, 2)      14          input_28[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2a (BatchNormalizati (None, 2, 2, 2)      8           res1a_branch2a[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "activation_67 (Activation)      (None, 2, 2, 2)      0           bn1a_branch2a[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2b (Conv2D)         (None, 2, 2, 4)      36          activation_67[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2b (BatchNormalizati (None, 2, 2, 4)      16          res1a_branch2b[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "activation_68 (Activation)      (None, 2, 2, 4)      0           bn1a_branch2b[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch2c (Conv2D)         (None, 2, 2, 6)      30          activation_68[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "res1a_branch1 (Conv2D)          (None, 2, 2, 6)      42          input_28[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch2c (BatchNormalizati (None, 2, 2, 6)      24          res1a_branch2c[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "bn1a_branch1 (BatchNormalizatio (None, 2, 2, 6)      24          res1a_branch1[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "add_22 (Add)                    (None, 2, 2, 6)      0           bn1a_branch2c[0][0]              \n",
    "                                                                 bn1a_branch1[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "activation_69 (Activation)      (None, 2, 2, 6)      0           add_22[0][0]                     \n",
    "==================================================================================================\n",
    "Total params: 194\n",
    "Trainable params: 158\n",
    "Non-trainable params: 36\n",
    "__________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para armar ResNet50\n",
    "\n",
    "Con estos bloques es posible armar ResNet50. La siguiente imagen demuestra la arquitectura en detalle. \"ID BLOCK\" significa \"Identity Block\", \"ID BLOCK x3\" significa que debemos apilar 3 identity blocks juntos.\n",
    "\n",
    "<img src=\"resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **ResNet-50** </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(224, 224, 3),\n",
    "             classes=1000,\n",
    "             **kwargs):\n",
    "    \"\"\"\n",
    "    Instancia la arquitectura ResNet50.\n",
    "    \n",
    "    # Argumentos\n",
    "        input_tensor: tensor de Keras opcional para usar de imagen de entrada al modelo\n",
    "        optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: tiene que tener 3 canales de entrada\n",
    "        classes: número opcional de clases en las que clasificar las imágenes\n",
    "\n",
    "    # Retorna\n",
    "        Un modelo de Keras\n",
    "    \"\"\"\n",
    "\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    \n",
    "    if backend.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x = layers.Dense(classes, activation='softmax', name='fc1000')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    \n",
    "    inputs = img_input\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name='resnet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparar la estructura con la ResNet en keras\n",
    "model = ResNet50()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
